<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Econometria I</title>
    <meta charset="utf-8" />
    <meta name="author" content="PhD.(C) Orlando Joaqui-Barandica" />
    <meta name="date" content="2023-01-01" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/rladies.css" rel="stylesheet" />
    <link href="libs/remark-css/rladies-fonts.css" rel="stylesheet" />
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/shareon/shareon.min.css" rel="stylesheet" />
    <script src="libs/shareon/shareon.min.js"></script>
    <link href="libs/xaringanExtra-shareagain/shareagain.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-shareagain/shareagain.js"></script>
    <link href="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <link href="libs/font-awesome/css/all.min.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.min.css" rel="stylesheet" />
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
    </script>
    <style>
    .mjx-mrow a {
      color: black;
      pointer-events: none;
      cursor: default;
    }
    </style>
    <link rel="stylesheet" href="fonts_mtheme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">





class: inverse, left, bottom
background-image: url("img/fondo.jpg")
background-size: cover


# **Econometria I**
----

## **&lt;br/&gt; Regresión Lineal Múltiple**

### PhD.(C) Orlando Joaqui-Barandica
### 2023





---
name: hola
class: inverse, middle, center

&lt;img style="border-radius: 60%;" src="img/jave.jpg"
width="150px"
/&gt;

# Pontificia Universidad Javeriana de Cali

--

## Programa de Economía
---




.pull-left[

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

&lt;img src="img/gif1.gif" width="110%" /&gt;
]

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;


.pull-right[
# Orlando Joaqui-Barandica
### [www.joaquibarandica.com](https://www.joaquibarandica.com)
 *PhD.(C) in Industrial Engineering* 
 
 *MSc. Applied Economics*
 
 *BSc. Statistics*
]

---



# Modelo lineal múltiple en forma matricial

- El modelo lineal múltiple es una herramienta común en la estadística y el análisis de datos.

- Permite relacionar una variable de interés con múltiples covariables.

- La forma matricial del modelo es particularmente útil para su análisis y resolución.



### El modelo lineal múltiple se puede expresar en forma matricial como:

  $$
  Y = X\beta + \epsilon
  $$



---

# Modelo lineal múltiple en forma matricial

$$
Y = X\beta + \epsilon
$$


`$$Y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} , \quad
X = \begin{bmatrix} 
1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\
1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np} \\
\end{bmatrix} , \quad
\beta = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p \end{bmatrix} , \quad
\epsilon = \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}$$`


Donde:

- Y es un vector de respuestas de longitud n.
- X es una matriz de covariables de dimensión n x p.
- beta es un vector de coeficientes de longitud p.
- epsilon es un vector de errores de longitud n.


---

# Solución de mínimos cuadrados

- La solución de mínimos cuadrados para `\(\beta\)` está dada por:

  $$
  \hat{\beta} = (X^\top X)^{-1} X^\top Y
  $$

- La matriz `\((X^\top X)^{-1} X^\top\)` se conoce como la **matriz pseudo-inversa** de `\(X\)`.

### Interpretación de los coeficientes

- Los coeficientes `\(\beta\)` representan el cambio en la respuesta `\(Y\)` esperado por cada cambio unitario en la covariable correspondiente, manteniendo las demás covariables constantes.
- Si `\(\beta_i &gt; 0\)`, entonces un aumento en la covariable `\(X_i\)` se asocia con un aumento en `\(Y\)`.
- Si `\(\beta_i &lt; 0\)`, entonces un aumento en la covariable `\(X_i\)` se asocia con una disminución en `\(Y\)`.
- Si `\(\beta_i = 0\)`, entonces la covariable `\(X_i\)` no está asociada con una variación en `\(Y\)`.

---


## Estimación de los modelos del parámetro

### El estimador de mínimos cuadrados

En la sección anterior, se introdujo el modelo de regresión lineal:

`$$Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots + \beta_pX_{ip} + \epsilon_i$$`

donde `\(Y_i\)` es la variable de respuesta para la observación `\(i\)`, `\(X_{i1}, X_{i2}, \ldots, X_{ip}\)` son las variables predictoras, `\(\beta_0, \beta_1, \beta_2, \ldots, \beta_p\)` son los parámetros desconocidos del modelo y `\(\epsilon_i\)` es el error aleatorio para la observación `\(i\)`.

El objetivo de la regresión lineal es encontrar los valores de los parámetros `\(\beta\)` que mejor ajusten los datos observados. En particular, queremos encontrar los valores de `\(\beta\)` que minimizan la suma de los residuos cuadrados:

`$$\sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (Y_i - \hat{Y_i})^2$$`

donde `\(\hat{Y_i}\)` es la predicción del modelo para la observación `\(i\)` y `\(e_i = Y_i - \hat{Y_i}\)` es el residuo correspondiente.

---

#### Estimador de mínimos cuadrados

La solución de mínimos cuadrados para el vector de parámetros `\(\beta\)` se obtiene minimizando la suma de los residuos cuadrados. La solución es:

`$$\hat{\beta} = (X^TX)^{-1}X^TY$$`

donde `\(X\)` es la matriz de diseño de tamaño `\(n \times (p+1)\)` que contiene las variables predictoras y una columna de unos para el término constante, y `\(Y\)` es el vector de respuestas de tamaño `\(n \times 1\)`.

##### Demostración

Para demostrar que `\(\hat{\beta}\)` es el estimador de mínimos cuadrados del vector de parámetros `\(\beta\)`, primero escribimos la suma de los residuos cuadrados en términos de `\(\beta\)` y `\(\hat{\beta}\)`:

`$$\sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (Y_i - \hat{Y_i})^2 = \sum_{i=1}^{n} (Y_i - X_i\hat{\beta})^2$$`

donde `\(X_i\)` es la fila `\(i\)` de la matriz de diseño `\(X\)`. Para minimizar esta expresión con respecto a `\(\hat{\beta}\)`, tomamos su derivada e igualamos a cero:

`$$\frac{\partial}{\partial \hat{\beta}} \sum_{i=1}^{n} (Y_i - X_i\hat{\beta})^2 = 0$$`
---

### Matricialmente:

`$$\sum_{i=1}^{n} \epsilon_i ^2 = \epsilon^T\epsilon = (y-X\beta)^T(y-X\beta)$$`

$$= y^Ty - \beta^TX^Ty - y^TX\beta + \beta X^TX\beta $$

Dado que.. `\((y^TX\beta)^T = \beta^TX^Ty\)` Entonces...

$$= y^Ty - 2 \beta^TX^Ty  + \beta X^TX\beta $$

La derivada de lo anterior, respecto a `\(\beta\)` e igualado a cero. Da como resultado las ecuaciones normales de mínimos cuadrados.

`$$X^TX\hat{\beta} = X^Ty$$`
Despejando...

`$$\hat{\beta} = (X^TX)^{-1} X^Ty$$`

---

# En R


```r
# Generar datos de ejemplo
set.seed(123)
n &lt;- 100
p &lt;- 3
X &lt;- matrix(rnorm(n * p), n, p)
beta &lt;- c(1, 2, -1, 0.5)
eps &lt;- rnorm(n)
Y &lt;- X %*% beta[-1] + beta[1] + eps

# Estimación por mínimos cuadrados
X_design &lt;- cbind(1, X)
beta_hat &lt;- solve(t(X_design) %*% X_design) %*% t(X_design) %*% Y
beta_hat
```

---

# Propiedades de los estimadores


`$$\begin{aligned}
E(\hat{\beta}) &amp;= E\left[(X^TX)^{-1}X^TY\right]\ \\
&amp;= E\left[(X^TX)^{-1}X^T(X\beta+\epsilon)\right]\ \\
&amp;= E\left[(X^TX)^{-1}X^TX\beta + (X^TX)^{-1}X^T\epsilon\right]\ \\
&amp;= \beta + (X^TX)^{-1}X^TE(\epsilon)\ \\
&amp;= \beta + (X^TX)^{-1}X^T\cdot 0\ \\
&amp;= \beta
\end{aligned}$$`


`$$\begin{aligned}
Var(\hat{\beta}) &amp;= E[(\hat{\beta}-E(\hat{\beta}))(\hat{\beta}-E(\hat{\beta}))^T]\ \\
&amp;= E[(X^TX)^{-1}X^T(\epsilon\epsilon^T)X(X^TX)^{-1}]\ \\
&amp;= (X^TX)^{-1}X^TE(\epsilon\epsilon^T)X(X^TX)^{-1}\ \\
&amp;= (X^TX)^{-1}X^T(\sigma^2I)X(X^TX)^{-1}\ \\
&amp;= \sigma^2(X^TX)^{-1} 
\end{aligned}$$`

donde usamos que `\(E(\epsilon\epsilon^T) = \sigma^2I\)`, debido a que el término de error `\(\epsilon\)` se asume que es independiente e idénticamente distribuido con una distribución normal con media cero y varianza `\(\sigma^2\)`. Además, usamos que `\((X^TX)^{-1}(X^T)^T=(X^TX)^{-1}X^TX=I\)`.



---

# Propiedades de los estimadores

`$$Var(\hat{\beta}) = \sigma^2 (X^T X)^{-1} = \begin{bmatrix}
\text{Var}(\hat{\beta}_1) &amp; \text{Cov}(\hat{\beta}_1, \hat{\beta}_2) &amp; \cdots &amp; \text{Cov}(\hat{\beta}_1, \hat{\beta}_p)\\
\text{Cov}(\hat{\beta}_2, \hat{\beta}_1) &amp; \text{Var}(\hat{\beta}_2) &amp; \cdots &amp; \text{Cov}(\hat{\beta}_2, \hat{\beta}_p)\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
\text{Cov}(\hat{\beta}_p, \hat{\beta}_1) &amp; \text{Cov}(\hat{\beta}_p, \hat{\beta}_2) &amp; \cdots &amp; \text{Var}(\hat{\beta}_p)
\end{bmatrix}$$`



---

## Supuesto de linealidad

El modelo de regresión lineal clásico con matrices se puede expresar como:

`$$Y = X\beta + \epsilon$$`

Donde:

- `\(Y\)` es un vector `\(n \times 1\)` de las observaciones de la variable dependiente.
- `\(X\)` es una matriz `\(n \times k\)` de las observaciones de las variables independientes.
- `\(\beta\)` es un vector `\(k \times 1\)` de los coeficientes de regresión.
- `\(\epsilon\)` es un vector `\(n \times 1\)` de los errores.

Este supuesto establece que la relación entre la variable independiente y la variable dependiente debe ser lineal.

---

## Supuesto de homocedasticidad

Este supuesto establece que los errores del modelo deben ser homocedásticos, lo que significa que la matriz de covarianza de los errores debe ser constante en toda la gama de valores de la variable independiente.

Matricialmente, se expresa como:

`$$Var(\epsilon) = \sigma^2I_n$$`

Donde:

- `\(Var(\epsilon)\)` es la matriz de covarianza de los errores.
- `\(\sigma^2\)` es la varianza común de los errores.
- `\(I_n\)` es la matriz identidad de orden `\(n\)`.

---

## Supuesto de normalidad

Este supuesto establece que los errores del modelo deben seguir una distribución normal.

Matricialmente, se expresa como:

`$$\epsilon \sim N_n(0, \sigma^2I_n)$$`

Donde:

- `\(N_n\)` es la distribución normal multivariada de orden `\(n\)`.

---

## Supuesto de independencia

Este supuesto establece que los errores del modelo deben ser independientes entre sí.

Matricialmente, se expresa como:

`$$Cov(\epsilon) = \sigma^2I_n$$`

Donde:

- `\(Cov(\epsilon)\)` es la matriz de covarianza de los errores.
- `\(\sigma^2\)` es la varianza común de los errores.
- `\(I_n\)` es la matriz identidad de orden `\(n\)`.

---



class: center, inverse

###  Es importante verificar estos supuestos antes de utilizar el modelo, ya que la violación de uno o varios de ellos puede afectar la precisión de los coeficientes estimados y la validez de las inferencias realizadas.


---


# Además...

Matricialmente, podemos expresar que el valor esperado de cada `\(u_i\)` es igual a cero condicionado a todas las observaciones de la matriz `\(X\)` como:




$$ E(\epsilon|X) = E(Y - X\beta|X) = E(Y|X) - E(X\beta|X) = X\beta - X\beta = 0 $$ 


.pull-left[

Donde:

- `\(E(\epsilon|X)\)` es el valor esperado condicionado de los errores.
- `\(Y\)` es un vector `\(n \times 1\)` de las observaciones de la variable dependiente.
- `\(X\)` es una matriz `\(n \times k\)` de las observaciones de las variables independientes.
- `\(\beta\)` es un vector `\(k \times 1\)` de los coeficientes de regresión.
- `\(\epsilon\)` es un vector `\(n \times 1\)` de los errores.
- `\(u_i\)` es el i-ésimo error del modelo.

]

.pull-right[

`$$E(\epsilon|X) = \begin{bmatrix} E(\epsilon_1|X) \\ E(\epsilon_2|X) \\ \vdots \\ E(\epsilon_n|X) \end{bmatrix} = \begin{bmatrix} 0 \\  0 \\  \vdots \\  0 \end{bmatrix} = 0$$` 


]

---

# Matriz de varianzas y covarianzas de los errores

La matriz de varianzas y covarianzas del vector de errores `\(\epsilon\)` se define como:

`$$Var(\epsilon)=\sigma^2I_n$$`

donde `\(\sigma^2\)` es la varianza común de los errores y `\(I_n\)` es la matriz identidad de orden `\(n \times n\)`.


Podemos demostrar esto de forma matricial:


Primero, consideramos la siguiente propiedad de matrices:

`$$Var(Ax) = AVar(x)A^T$$`
donde `\(A\)` es una matriz de constantes y `\(x\)` es un vector de variables aleatorias.

Luego, podemos expresar el vector de errores `\(\epsilon\)` como:

`$$\epsilon = y - X\beta$$`
donde `\(y\)` es el vector de variables respuesta, `\(X\)` es la matriz de variables explicativas y `\(\beta\)` es el vector de coeficientes de regresión.

---

Tomando `\(A = X\)`, podemos escribir:

`$$Var(X\beta) = XVar(\beta)X^T$$`
Como el vector de errores es `\(\epsilon = y - X\beta\)`, tenemos:

`$$Var(\epsilon) = Var(y - X\beta) = Var(y)- Var(X\beta) - 2Cov(y, X\beta)$$`
donde `\(Cov(y, X\beta)\)` es la covarianza entre `\(y\)` y `\(X\beta\)`.


Como se asume que los errores `\(\epsilon\)` tienen varianza constante y son no correlacionados, tenemos que `\(Var(y) = \sigma^2 I_n\)` y `\(Cov(y, X\beta) = 0\)`. Entonces, la ecuación anterior se reduce a:


`$$Var(\epsilon) = \sigma^2 I_n + XVar(\beta)X^T - 2(0)$$`
Entonces, reemplazamos `\(\sigma^2 I_n\)` por `\(\sigma^2(X^TX)^{-1}(X^TX)\)`:


.pull-left[
donde utilizamos que `\(Var(y) = \sigma^2 I_n\)` y `\(Cov(y, X\beta) = 0\)`, por simplicidad teórica se asume `\(Var(\beta) = 0\)`. Si ahora asumimos que los errores son no correlacionados, es decir, que `\(Cov(\epsilon_i, \epsilon_j) = 0\)` para `\(i \neq j\)`, entonces la matriz de varianzas y covarianzas del error se reduce a:
]

.oull-right[
`$$Var(\epsilon) = \sigma^2 I_n = \begin{pmatrix}
\sigma^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma^2
\end{pmatrix}$$`
]

---


class: inverse, center, middle
background-color: #122140

.pull-left[

.center[
&lt;br&gt;&lt;br&gt;

# Gracias!!!

&lt;br&gt;



### ¿Preguntas?

&lt;br&gt;


&lt;img src="img/qr-code.png" width="49%" style="display: block; margin: auto;" /&gt;


]


]


.pull-right[

&lt;br&gt; 
&lt;br&gt; 
&lt;img style="border-radius: 50%;" src="img/avatar.png"
width="150px"
/&gt;

### [www.joaquibarandica.com](https://www.joaquibarandica.com)

<i class="fa fa-envelope" role="presentation" aria-label="envelope icon"></i> orlando.joaqui@javerianacali.edu.co

&lt;img src="img/Logo.jpg" width="120%"&gt;

]


&lt;br&gt;&lt;br&gt;&lt;br&gt;









    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
