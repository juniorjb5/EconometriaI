---
title: "Econometria I"
subtitle: "<br/> Regresión Lineal Simple"
author: "PhD.(C) Orlando Joaqui-Barandica"
institute: "Pontificia Universidad Javeriana de Cali"
date: "2023"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: 
      - default
      - rladies
      - rladies-fonts
      - fonts_mtheme.css
    includes:
      in_header: "mathjax-equation-numbers.html"
    seal: false  
    nature: 
      ratio: 16:9
      beforeInit: "macros.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include = FALSE}
library(knitr)                              # paquete que trae funciones utiles para R Markdown
library(tidyverse)                          # paquete que trae varios paquetes comunes en el tidyverse
library(datos)                              # paquete que viene con datos populares traducidos al español :)
library(shiny)
# opciones predeterminadas
knitr::opts_chunk$set(echo = FALSE,         # FALSE: los bloques de código NO se muestran
                      dpi = 300,            # asegura gráficos de alta resolución
                      warning = FALSE,      # los mensajes de advertencia NO se muestran
                      error = FALSE)        # los mensajes de error NO se muestran


options(htmltools.dir.version = FALSE)

library(xaringan)

```

class: inverse, left, bottom
background-image: url("img/fondo.jpg")
background-size: cover


# **`r rmarkdown::metadata$title`**
----

## **`r rmarkdown::metadata$subtitle`**

### `r rmarkdown::metadata$author`
### `r rmarkdown::metadata$date`

```{r xaringanExtra-share-again, echo=FALSE}
xaringanExtra::use_share_again()
```

```{r xaringanExtra-clipboard, echo=FALSE}
xaringanExtra::use_clipboard()
```

---
name: hola
class: inverse, middle, center

<img style="border-radius: 60%;" src="img/jave.jpg"
width="150px"
/>

# Pontificia Universidad Javeriana de Cali

--

## Programa de Economía
---




.pull-left[

<br><br><br><br><br>

```{r echo=FALSE, out.width = "110%" }
knitr::include_graphics("img/gif1.gif")
```
]

<br><br><br><br><br>


.pull-right[
# Orlando Joaqui-Barandica
### [www.joaquibarandica.com](https://www.joaquibarandica.com)
 *PhD.(C) in Industrial Engineering* 
 
 *MSc. Applied Economics*
 
 *BSc. Statistics*
]

---

name: menu
background-image: url("img/back2.jpg")
background-size: cover
class: left, middle, inverse

# Contenido

----


.pull-left[

### `r icon("dice-d6")` [FRP y FRM](#FRP)

### `r icon("database")` [X fija o aleatoria?](#fija)

### `r icon("dice-d6")` [Precisión de los estimadores](#precision)

]


.pull-right[



### `r icon("upload")` [Estimación de $\sigma^2$](#varianza)


### `r icon("sort-numeric-up")` [Distr. de prob. de $u_i$](#ui)



]

---


name: FRP
class: inverse, center, middle

# `r icon("dice-d6")`
# FRP y FRM
----

.right[
.bottom[
####  [`r icon("bell")`](#menu)
]
]

---

# Recordemos...


### Función de regresión poblacional (FRP)

Es una función lineal de $x$. Dado cualquier valor de $x$, la distribución de $y$ está centrada en $E(y|x)$ 

$$E(y|x) =\beta_0 + \beta_1x$$


### Función de regresión muestral (FRM)

Una vez que se han determinado las estimaciones por MCO del intercepto y de la pendiente, se obtiene la línea de regresión de MCO.

$$ \hat{y} = \hat{\beta_0} + \hat{\beta_1}x$$


.orange[**El intercepto,**] $\hat{\beta_0}$, es el valor predicho de $y$ cuando $x = 0$, aunque en algunos casos no tiene sentido hacer $x = 0$. En estas situaciones, $\hat{\beta_0}$ no tiene gran interés por sí misma.


> .green[Es importante recordar que la FRP es algo fijo, pero desconocido, de la población. Dado que la FRM se obtiene a partir de una muestra de datos, con una nueva muestra se obtendrán una pendiente e intercepto diferentes.]



---



name: fija
class: inverse, center, middle

# `r icon("dice-d6")`
# $X_i$ Fija o Aleatoria?
----

.right[
.bottom[
####  [`r icon("bell")`](#menu)
]
]

---

# $X_i$ Fija o Aleatoria?

Al hacer cálculos estadísticos, condicionar sobre los valores muestrales de la variable independiente es lo mismo que tratar a las $x_i$ como fijas en muestras repetidas, lo cual se entiende como sigue:

<br>

- Primero se escogen n valores muestrales para $x_1, x_2, ..., x_n$. (Éstos pueden repetirse.)     - Dados estos valores, se obtiene después una muestra de $y$. 
- A continuación, se obtiene otra muestra de $y$, usando los mismos valores para $x_1, x_2, ..., x_n$. 
- Después se obtiene otra muestra de $y$, usando los mismos $x_1, x_2, ..., x_n$. Y así sucesivamente.


<br>
<br>

.pull-left[

.center[
### La situación de que las $x_i$ sean fijas en muestras repetidas no es muy realista en los contextos no experimentales. 
]
]


.pull-rigth[

.center[.orange[**Por ejemplo,** al muestrear individuos para el caso del salario y la educación, no tiene mucho sentido pensar en elegir de antemano los valores de *educ* y después muestrear individuos que tengan esos determinados niveles de educación.]]

]



---

# $X_i$ Fija o Aleatoria?


> Una vez que se supone que $E(u|x) = 0$, y que se tiene un muestreo aleatorio, **no cambia
nada en los cálculos al tratar a las $x_i$ como no aleatorias o aleatorias.** 

<br>

----

* .orange[Si las variables independientes $X$ son aleatorias, entonces se dice que estamos trabajando con un modelo de regresión lineal probabilístico. Esto significa que la relación entre las variables $X$ e $Y$ es modelada a través de una distribución de probabilidad. 

En este caso, la incertidumbre sobre la relación entre $X$ e $Y$ es representada a través de la varianza de la distribución de probabilidad.]

----


.pull-left[
Si las variables independientes X son aleatorias, entonces se espera que la esperanza condicional $E(u|x)$ sea cero para cualquier valor de x. Esto significa que, dada una variable independiente X, la media de los errores esperados en los valores de Y asociados con ese valor de X es cero. 
]

.pull-right[

> Para decidir si con un análisis de regresión simple se van a obtener estimadores insesgados, es crucial pensar en los términos del supuesto 3.


$$E(u|x) = 0$$

]

---



name: precision
class: inverse, center, middle

# `r icon("dice-d6")`
# Precisión de los estimadores
----

.right[
.bottom[
####  [`r icon("bell")`](#menu)
]
]

---

# Precisión de los estimadores

Los estimadores po MCO tinen algunas propiedades importantes. Por ejemplo son combinaciones lineales de las observaciones de $y_i$. Por ejemplo:

.pull-left[


### Esperanza de $\beta_1$


$$\hat{\beta_1} = \frac{S_{xy}}{S_{xx}} = \sum_{i=1}^{n} c_i y_i$$
donde, 

- $S_{xy}$ es la covarianza entre $x$ y $y$
- $S_{xx}$ es la varianza entre $x$
- $c_i = (x_i - \bar{x}) / S_{xx}$ para $i = 1, 2, ..., n$


Los estimadores $\hat{\beta_0}$ y $\hat{\beta_1}$ son estimadores insesgados de los parámetros $\beta_0$ y $\beta_1$ del modelo.

]



.pull-right[


$$
\begin{align*}
E(\hat{\beta_1}) & = E \left( \sum_{i=1}^{n} c_i y_i\right) = \sum_{i=1}^{n} c_i E(y_i) \\
 & = \sum_{i=1}^{n} c_i (\beta_0 + \beta_1 x_i) \\
 & = \beta_0 \sum_{i=1}^{n} c_i + \beta_1 \sum_{i=1}^{n} c_i x_i
\end{align*}
$$
<br>

.orange[**Por propiedades:**] $\sum_{i=1}^{n} c_i = 0$, dado que el numerador $\sum_{i=1}^{n} (x_i - \bar{x})$ muestra la suma de las desviaciones alrededor de la media, lo cual por propiedades de la varianza es igual a 0.
]


---

# Precisión de los estimadores


.pull-left[


$$
\begin{align*}
E(\hat{\beta_1}) & = E \left( \sum_{i=1}^{n} c_i y_i\right) = \sum_{i=1}^{n} c_i E(y_i) \\
 & = \sum_{i=1}^{n} c_i (\beta_0 + \beta_1 x_i) \\
 & = \beta_0 \sum_{i=1}^{n} c_i + \beta_1 \sum_{i=1}^{n} c_i x_i
\end{align*}
$$

<br>

.orange[**Por propiedades:**] $\sum_{i=1}^{n} c_i = 0$, dado que el numerador $\sum_{i=1}^{n} (x_i - \bar{x})$ muestra la suma de las desviaciones alrededor de la media, lo cual por propiedades de la varianza es igual a 0.
]


.pull-right[


.orange[**Por propiedades:**] $c_i$ ejerce como un ponderador en términos de $x_i$. Si se multiplica el ponderador por $x_i$ y se realiza la sumatoria obligatoriamente será 1.

$$
\begin{align*}
\sum c_i x_i = & \frac{(x_i - \bar{x})} {S_{xx}} x_i = \frac{(x_i - \bar{x})} {\sum (x_i - \bar{x})^2}x_i \\
\sum c_i x_i = & 1
\end{align*}
$$

<br>

.green[**De tal manera:**]

$$
\begin{align*}
E(\hat{\beta_1}) & = \beta_0 \sum_{i=1}^{n} c_i + \beta_1 \sum_{i=1}^{n} c_i x_i \\
& = \beta_0 (0) + \beta_1 (1) \\
& = \beta_1
\end{align*}
$$

]


---

# Precisión de los estimadores


.pull-left[

### Varianza de $\beta_1$


$$
\begin{align*}
Var (\hat{\beta_1}) = & Var \left( \sum_{i=1}^{n} c_i y_i \right) \\
 = & \sum_{i=1}^{n} c_i ^2 Var(y_i)
\end{align*}
$$
Ya que las observaciones $y_i$ son no correlacionadas, por lo que la varianza de la suma es igual a la suma de las varianzas. La varianza de cada término en la suma es $c_i ^2 Var(y_i)$ y hemos supuesto que $Var(y_i) = \sigma^2$.


]

.pull-right[

En consecuencia,

$$
\begin{align*}
Var (\hat{\beta_1}) = & \sigma^2 \sum_{i=1}^{n} c_i^2  \\
 = & \frac{\sigma^2 \sum (x_i - \bar{x})^2}{S^2_{xx}} \\
 = & \frac{\sigma^2}{S_{xx}} 
\end{align*}
$$
Es decir,

$$Var (\hat{\beta_1}) = \frac{\sigma^2}{\sum (x_i - \bar{x})^2}$$


]




---


# Precisión de los estimadores


## Error estándar 

Las estimaciones de mínimos cuadrados son función de los datos muestrales. Pero, como es probable que los datos cambien entre una muestra y otra, los valores estimados cambiarán `ipso facto`.


Se requiere una medida de precisión de los estimadores $\hat{\beta_i}$.


.center[
### En estadística la precisión de un valor estimado se mide por su error estándar (ee)
]



.pull-left[

.orange[**El error estándar** no es otra cosa que la desviación estándar de la distribución muestral del estimador, y la distribución muestral de un estimador es tan sólo una probabilidad o distribución de frecuencias del estimador, es decir, una distribución del conjunto de valores del estimador obtenidos de todas las muestras posibles de igual tamaño de una población dada.]
]

.pull-right[

$$Var (\hat{\beta_1}) = \frac{\sigma^2}{\sum (x_i - \bar{x})^2}$$

$$ee(\hat{\beta_1}) = \sqrt{Var(\hat{\beta_1}}) = \frac{\sigma}{\sqrt{\sum (x_i - \bar{x})^2}}$$

]


---

class: center, middle

# Para el caso de $\hat{\beta_0}$ se sigue el mismo proceso.

---


name: varianza
class: inverse, center, middle

# `r icon("dice-d6")`
# Estimación de $\sigma^2$
----

.right[
.bottom[
####  [`r icon("bell")`](#menu)
]
]

---

# Estimación de $\sigma^2$


Además de estimar $\beta_0$ y $\beta_1$ se requiere un estimado de $\sigma^2$ para probar hipótesis y formar estimados de intervalo pertinentes al modelo de regresión.

> El estimado de $\sigma^2$ se obtiene a partir de la suma de cuadrados del error SCE

$$SCE = \sum_{i=1}^n u_i^2 = \sum_{i=1}^n (y_i - \hat{y_i})^2$$

Sea entonces:

.pull-left[

$$ \hat{\sigma}^2 = \frac{ \sum_{i=1}^n u_i^2 } {n-2} = \frac{SCE}{n-2} = CME$$

donde, $n-2$ son lo grados de libertad necesarios para la estimación del modelo, equivale a $n:$ .green[Num. de Observaciones.] $2:$ .orange[Num. de parámetros estimados] $(\hat{\beta_0},\hat{\beta_1})$.
A lo anterior, se le conoce como el $CME$ **(Cuadrado medio del error).**

]

.pull-right[

### Error estándar de la estimación (o regresión)

$$\hat{\sigma} = \sqrt{CME} = \sqrt{\frac{SCE}{n-2}}$$

No es más que la desviación estándar de los valores $Y$ alrededor de la línea de regresión estimada.

]



---

# Estimación de $\sigma^2$

Uno de los objetivos de la regresión lineal es estimar los coeficientes $\beta_0$ y $\beta_1$ a partir de los datos. En particular, la varianza estimada del término de error se define como:

$$\hat{\sigma}^2 = \frac{SCE}{n-2}$$

donde $SCE$ es la suma de los errores al cuadrado (residuos) y $n$ es el tamaño de la muestra.


### En un modelo de regresión lineal simple, se puede demostrar que la varianza poblacional del término de error es:

$$Var(u_i) = \sigma^2$$


---

# Estimación de $\sigma^2$

Para demostrar que la .orange[**esperanza de la varianza estimada es igual a la varianza poblacional**], consideremos la expresión:

$$E[\hat{\sigma}^2] = E\left[\frac{SCE}{n-2}\right]$$

Aplicando la ley de las esperanzas condicionales, podemos expresar la esperanza de $SCE$ como:

$$E[SCE] = (n-2)\sigma^2$$

Esto se puede demostrar utilizando la definición de $SCE$:

$$SCE = \sum_{i=1}^{n} (y_i - \hat{y_i})^2$$

---

# Estimación de $\sigma^2$

.pull-left[

Luego, aplicando la ley de las esperanzas condicionales, tenemos:

$$E[SCE] = E\left[\sum_{i=1}^{n} (y_i - \hat{y_i})^2\right]$$

$$=\sum_{i=1}^{n} E\left[(y_i - \hat{y_i})^2\right]$$

$$=\sum_{i=1}^{n} \left[Var(u_i) + E(\hat{y_i} - y_i)^2\right]$$

$$=\sum_{i=1}^{n} \left[\sigma^2 + E(\hat{y_i} - y_i)^2\right]$$

$$=(n-2)\sigma^2$$

]

.pull-right[

donde se ha utilizado que la media de los errores es cero y la insesgadez de los estimadores.

Sustituyendo este resultado en la expresión original de la esperanza de $s^2$, obtenemos:

$$E[\hat{\sigma}^2] = E\left[\frac{SCE}{n-2}\right] = \frac{E[SCE]}{n-2} = \sigma^2$$

> .green[**Por lo tanto, se puede demostrar que la esperanza de la varianza estimada es igual a la varianza poblacional en un modelo de regresión lineal simple.**]


]

---

# Ley de las esperanzas condicionales

Para demostrar que $E[s^2] = \sigma^2$, utilizamos la ley de las esperanzas condicionales, que nos dice que:

$$E[X] = E[E[X|Y]]$$

donde $X$ y $Y$ son variables aleatorias.

.orange[**Primero,**] definimos $SCE$ como la suma de los errores al cuadrado:

$$SCE = \sum_{i=1}^{n} (y_i - \hat{y_i})^2$$
donde $\hat{y_i}$ es el valor predicho por el modelo para la observación $i$.

Luego, utilizamos la ley de las esperanzas condicionales para expresar $E[SCE]$ en términos de $E[u_i]$, que es la esperanza del término de error. En particular, tenemos:

$$E[SCE] = E\left[\sum_{i=1}^{n} (y_i - \hat{y_i})^2\right] = \sum_{i=1}^{n} E\left[(y_i - \hat{y_i})^2\right]$$

---

# (Algo de explicación...)

La segunda igualdad se debe a la definición de la esperanza condicional. Para calcular la esperanza de $(y_i - \hat{y_i})^2$, podemos utilizar la esperanza condicional de $(y_i - \hat{y_i})^2$ dado $y_1, y_2, ..., y_n$. Es decir:

$$E[(y_i - \hat{y_i})^2] = E[(y_i - \hat{y_i})^2 \mid y_1, y_2, ..., y_n]$$

Sin embargo, en la demostración, simplificamos aún más esta esperanza condicional utilizando la relación entre $y_i$ y $\hat{y_i}$ en el modelo de regresión lineal simple. En particular, sabemos que $\hat{y_i} = \beta_0 + \beta_1 x_i$, por lo que podemos escribir:

$$(y_i - \hat{y_i})^2 = (y_i - \beta_0 - \beta_1 x_i)^2 = u_i^2$$


Donde $u_i$ es el término de error asociado a la observación $i$. Por lo tanto, podemos reescribir la esperanza de $(y_i - \hat{y_i})^2$ como la esperanza de $u_i^2$:

$$E[(y_i - \hat{y_i})^2] = E[u_i^2 \mid y_1, y_2, ..., y_n]$$

---

# (Algo de explicación...)


Luego, utilizamos la ley de las esperanzas condicionales nuevamente para descomponer la esperanza de $u_i^2$ en términos de $E[u_i]$ y $Var(u_i)$, lo que nos llevó a la expresión final:

$$E[(y_i - \hat{y_i})^2] = E[u_i^2 \mid y_1, y_2, ..., y_n] = Var(u_i \mid y_1, y_2, ..., y_n) + E[u_i \mid y_1, y_2, ..., y_n]^2 = \sigma^2 + E[u_i]^2$$


La expresión $E[(y_i - \hat{y_i})^2] = E[u_i^2 \mid y_1, y_2, ..., y_n]$ proviene de la definición del término de error $u_i = y_i - \hat{y_i}$.

Para obtener la segunda expresión, utilizamos la definición de varianza condicional y la ley de las esperanzas condicionales. Por definición, la varianza condicional de $u_i$ dado $y_1, y_2, ..., y_n$ es:


$$Var(u_i \mid y_1, y_2, ..., y_n) = E[u_i^2 \mid y_1, y_2, ..., y_n] - E[u_i \mid y_1, y_2, ..., y_n]^2$$

---

# (Algo de explicación...)

Por lo tanto,

$$E[u_i^2 \mid y_1, y_2, ..., y_n] = Var(u_i \mid y_1, y_2, ..., y_n) + E[u_i \mid y_1, y_2, ..., y_n]^2$$

y reemplazando en la expresión original, obtenemos:

$$E[(y_i - \hat{y_i})^2] = E[u_i^2 \mid y_1, y_2, ..., y_n] = Var(u_i \mid y_1, y_2, ..., y_n) + E[u_i \mid y_1, y_2, ..., y_n]^2$$

Esta última expresión muestra que la esperanza del cuadrado del término de error se puede descomponer en la varianza condicional del término de error más la media condicional del término de error al cuadrado.

---

# Ley de las esperanzas condicionales





En el último paso, utilizamos la propiedad de linealidad de la esperanza.

<br>

A continuación, utilizamos la **ley de las esperanzas condicionales** de nuevo para expresar $E[(y_i - \hat{y_i})^2]$ en términos de $Var(u_i)$ y $E[(x_i - \bar{x})(y_i - \bar{y})]$, que es la covarianza entre $x_i$ e $y_i$. En particular, tenemos:

$$E[(y_i - \hat{y_i})^2] = Var(u_i) + E[(\hat{y_i} - y_i)^2]$$

La primera parte de esta expresión se puede demostrar utilizando la definición de $Var(u_i)$, que es la varianza poblacional del término de error.


---

# Ley de las esperanzas condicionales

La segunda parte de la expresión se puede demostrar utilizando la expansión de $(\hat{y_i} - y_i)^2$ y algunas propiedades de los estimadores de la regresión lineal simple.

Por último, sustituimos estas expresiones en la expresión original para $E[SCE]$ y obtenemos:

$$E[SCE] = \sum_{i=1}^{n} \left[Var(u_i) + E[(\hat{y_i} - y_i)^2]\right]$$

$$= nVar(u_i) + \sum_{i=1}^{n} E[(\hat{y_i} - y_i)^2]$$

En la última expresión, utilizamos que la varianza del término de error es la misma para todas las observaciones.


Para completar la demostración, tenemos que demostrar que la suma de los errores al cuadrado $E[(\hat{y_i} - y_i)^2]$ es igual a $(n-2)\sigma^2$, donde $\sigma^2$ es la varianza poblacional del término de error.

Para hacerlo, podemos utilizar la ley de las esperanzas condicionales de nuevo y la expresión de $E[\hat{y_i}]$, que es la esperanza de $\hat{y_i}$.


---

# (Algo de explicación...)




Para demostrar que la suma de los errores al cuadrado $E[(\hat{y_i} - y_i)^2]$ es igual a $(n-2)\sigma^2$, podemos utilizar los siguientes pasos:

> *La siguiente notación es la misma, depende de la referencia: $CME \equiv MSE$*

.orange[**1. Primero,**] recordemos que la fórmula para el error cuadrático medio (MSE, por sus siglas en inglés) es:

$MSE = \frac{1}{n}\sum_{i=1}^{n}(\hat{y_i} - y_i)^2$

.orange[**2. Luego,**] podemos calcular el valor esperado de MSE de la siguiente manera:
$E[MSE] = E\left[\frac{1}{n}\sum_{i=1}^{n}(\hat{y_i} - y_i)^2\right]$

.orange[**3. Expandiendo**] el cuadrado en el término $(\hat{y_i} - y_i)^2$, podemos reescribir el MSE de la siguiente manera:
$MSE = \frac{1}{n}\sum_{i=1}^{n}(\hat{y_i}^2 - 2\hat{y_i}y_i + y_i^2)$


.orange[**4. Ahora,**] podemos tomar el valor esperado de cada uno de los términos en esta ecuación. Como asumimos que $\hat{y_i}$ es un estimador insesgado de $y_i$, tenemos que $E[\hat{y_i}] = y_i$, lo que significa que $E[\hat{y_i}^2] = Var(\hat{y_i}) + E[\hat{y_i}]^2 = Var(\hat{y_i}) + y_i^2$.

---

# (Algo de explicación...)

.orange[**5. Además,**] dado que $y_i$ es una variable aleatoria con una distribución de varianza $\sigma^2$, tenemos que $E[y_i^2] = Var(y_i) + E[y_i]^2 = \sigma^2 + y_i^2$.


.orange[**6. Sustituyendo**] estos valores esperados en la ecuación del paso 3, obtenemos:

$E[MSE] = \frac{1}{n}\sum_{i=1}^{n}(Var(\hat{y_i}) + Var(y_i))$

$E[MSE] = \frac{1}{n}\sum_{i=1}^{n}Var(\hat{y_i}) + \frac{1}{n}\sum_{i=1}^{n}Var(y_i)$


.orange[**7. Recordemos**] que el estimador de mínimos cuadrados ordinarios (OLS, por sus siglas en inglés) tiene una varianza de $\frac{\sigma^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}$ para cada coeficiente. Como asumimos que hay $k+1$ coeficientes (incluyendo el intercepto), la varianza de la estimación completa es $\sigma^2(X^TX)^{-1}$, donde $X$ es la matriz de datos y $X^TX$ es su producto matricial. Esto implica que:

$Var(\hat{y_i}) = \sigma^2\left[\frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\right]$

.orange[**8. Sustituyendo**] el valor de la varianza de $\hat{y_i}$ en la ecuación del paso 6, obtenemos:
$E[MSE] = \frac{\sigma^2}{n}\sum_{i=1}^{n}\left[\frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\right] + \frac{1}{n}\sum_{i=1}^{n}\sigma^2$

---


# (Algo de explicación...)

.orange[**9. Podemos simplificar**] la primera sumatoria al notar que $\sum_{i=1}^{n}(x_i - \bar{x})^2 = (n-1)S_x^2$, donde $S_x^2$ es la varianza muestral de los valores de $x$. Además, la segunda sumatoria es simplemente igual a $\sigma^2$. Por lo tanto, podemos escribir:
$E[MSE] = \sigma^2\left[\frac{1}{n} + \frac{(n-1)S_x^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\right] + \frac{\sigma^2}{n}$

.orange[**10. Podemos simplificar**] aún más la expresión en el primer corchete usando que $\frac{(n-1)S_x^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2} = \frac{n-1}{n-1} = 1$. Por lo tanto:
$E[MSE] = \sigma^2\left[\frac{1}{n-1}\right]\sum_{i=1}^{n}\left[\frac{n-1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}\right]$

.orange[**11. La expresión**] dentro de la sumatoria en el segundo corchete es la proporción de la varianza explicada por el modelo, es decir, $R_i^2$. Por lo tanto, podemos escribir:
$E[MSE] = \sigma^2\left[\frac{1}{n-1}\right]\sum_{i=1}^{n}R_i^2$

.orange[**12. Finalmente,**] como la suma de los cuadrados explicados (SSE) es igual a la suma de los cuadrados totales (SST) menos la suma de los cuadrados residuales (SSR), tenemos que $\sum_{i=1}^{n}R_i^2 = 1 - \frac{SSR}{SST}$. Como el modelo tiene $k$ coeficientes, tenemos que $SST = (n-1)\sigma^2$ y $SSR = (n-k)\sigma^2$, por lo que:
$E[MSE] = \sigma^2\left[\frac{1}{n-1}\right]\left[1 - \frac{(n-k)\sigma^2}{(n-1)\sigma^2}\right]$

$E[MSE] = \sigma^2\left[\frac{n-k}{n-1}\right]$

---

# (Algo de explicación...)


.orange[**13. Esta es la expresión que estábamos buscando.**] Si recordamos que $MSE$ es una estimación insesgada de $\sigma^2$, podemos reemplazar $E[MSE]$ por $\sigma^2$ en la ecuación anterior:
$\sigma^2 = \frac{n-k}{n-1}MSE$


.orange[**14. Ahora,**] podemos utilizar la definición de $MSE$ para obtener una expresión para la suma de los errores al cuadrado:
$MSE = \frac{1}{n-k}\sum_{i=1}^{n}(y_i - \hat{y_i})^2$

$E\left[\frac{1}{n-k}\sum_{i=1}^{n}(y_i - \hat{y_i})^2\right] = E[MSE] = \frac{n-k}{n-1}\sigma^2$


.orange[**15. Por lo tanto,**] la suma de los errores al cuadrado es igual a:
$E\left[\sum_{i=1}^{n}(y_i - \hat{y_i})^2\right] = (n-k)E[MSE]$

$E\left[\sum_{i=1}^{n}(y_i - \hat{y_i})^2\right] = (n-k)\frac{n-k}{n-1}\sigma^2$

$E\left[\sum_{i=1}^{n}(y_i - \hat{y_i})^2\right] = \frac{(n-k)^2}{n-1}\sigma^2$


---

# (Algo de explicación...)

.orange[**16. Si asumimos,**] que la media de la respuesta dependiente es cero y que los coeficientes del modelo son estimados sin sesgo, entonces tenemos que $k+1$ de los valores $y_i$ son estimados sin sesgo por el modelo. Entonces, podemos usar la definición de varianza insesgada para reemplazar $\sigma^2$ por $s^2$, la varianza muestral:

$E\left[\sum_{i=1}^{n}(y_i - \hat{y_i})^2\right] = \frac{(n-k)^2}{n-1}s^2$


.orange[**17. Ahora,**] podemos dividir ambos lados de la ecuación por $n-2$, que es el número de grados de libertad que quedan después de estimar los coeficientes del modelo. Obtenemos:
$E\left[\frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{n-2}\right] = \frac{(n-k)^2}{(n-1)(n-2)}s^2$

$E\left[\frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{n-2}\right] = \frac{n-k}{n-2}s^2$

.orange[**18. Recordando,**] que $s^2$ es una estimación insesgada de $\sigma^2$, podemos reemplazar $s^2$ por $\frac{SS_E}{n-2}$, donde $SS_E$ es la suma de los cuadrados del error:
$E\left[\frac{SS_E}{n-2}\right] = \frac{n-k}{n-2}\sigma^2$


.orange[**19. Como queríamos demostrar,**] la suma de los errores al cuadrado es igual a $(n-2)\sigma^2$ si asumimos que la media de la respuesta dependiente es cero, que los coeficientes del modelo son estimados sin sesgo, y que $s^2$ es una estimación insesgada de $\sigma^2$.


---

# Por otro lado... (Algo de explicación...)

La varianza de $\sigma^2$ estimado, denotada como $Var\left(\frac{SS_E}{n-2}\right)$, depende de la distribución de los errores. 

<br>

Bajo la suposición de que los errores son normales e independientes y tienen una varianza constante $\sigma^2$, podemos utilizar la distribución de la suma de cuadrados de una muestra aleatoria de una distribución normal para obtener la varianza de $\frac{SS_E}{n-2}$. En este caso, la varianza de $\frac{SS_E}{n-2}$ es:

$$Var\left(\frac{SS_E}{n-2}\right) = \frac{2\sigma^4}{n-2}$$

> Si los errores no son normales o no tienen una varianza constante, la distribución de la suma de cuadrados de los errores puede ser diferente, y la varianza de $\frac{SS_E}{n-2}$ puede ser diferente también.


---

# Covarianza de los estimadores

Aquí te dejo cómo calcular y visualizar la covarianza entre los coeficientes estimados en una regresión lineal simple en R usando la base de datos "mtcars".


.pull-left[

----

```{c1, warning=FALSE, message=FALSE, eval=FALSE, echo=TRUE}


# Cargar librería
library(tidyverse)

# Cargar base de datos mtcars
data(mtcars)

# Ajustar modelo de regresión lineal simple
model <- lm(mpg ~ wt, data = mtcars)

# Extraer coeficientes del modelo
coeficients <- coef(model)

# Mostrar coeficientes
coeficients



```

----

]




.pull-right[

.font70[
```{r, warning=FALSE, message=FALSE, eval=TRUE}

# Cargar librería
library(tidyverse)

# Cargar base de datos mtcars
data(mtcars)

# Ajustar modelo de regresión lineal simple
model <- lm(mpg ~ wt, data = mtcars)

# Extraer coeficientes del modelo
coeficients <- coef(model)

# Mostrar coeficientes
coeficients
```
]

Esta diapositiva muestra cómo ajustar un modelo de regresión lineal simple utilizando la base de datos "mtcars", y cómo extraer los coeficientes del modelo utilizando la función "coef". En este caso, la variable dependiente es "mpg" (millas por galón) y la variable independiente es "wt" (peso del auto).

]


---

# Covarianza de los estimadores


.pull-left[

----

```{c1, warning=FALSE, message=FALSE, eval=FALSE, echo=TRUE}


# Calcular matriz de covarianza de los coeficientes
covariance_matrix <- vcov(model)

# Mostrar matriz de covarianza
covariance_matrix



```

----

]




.pull-right[

.font70[
```{r, warning=FALSE, message=FALSE, eval=TRUE}

# Calcular matriz de covarianza de los coeficientes
covariance_matrix <- vcov(model)

# Mostrar matriz de covarianza
covariance_matrix

```
]

En esta diapositiva, calculamos la matriz de covarianza de los coeficientes estimados en el modelo de regresión lineal simple utilizando la función "vcov". Esta matriz muestra la varianza y la covarianza entre los coeficientes.


]

---


name: ui
class: inverse, center, middle

# `r icon("dice-d6")`
# Distribución de probabilidad de las perturbaciones $u_i$
----

.right[
.bottom[
####  [`r icon("bell")`](#menu)
]
]

---

# Modelo clásico de regresión lineal normal (MCRLN)

La llamada teoría clásica de la inferencia estadística consta de dos ramas, a saber: .orange[**estimación**] y .orange[**pruebas de hipótesis.**] Hasta el momento hemos estudiado el tema de la estimación de los parámetros del modelo de regresión lineal (con dos variables).



> El método MCO permitió estimar los parámetros de $\beta_0$, $\beta_1$ y $\sigma^2$


.left-column[

Los supuestos del *Modelo clásico de regresión lineal (MCRL)* permitieron conocer que:

$$\hat{\beta_0} \quad \hat{\beta_1} \quad \hat{\sigma}^2$$
tinene propiedades como: 

- Insesgamiento
- Varianza mínima
- Eficiencia


]


.right-column[

Pero la estimación es sólo la mitad de la batalla. Las pruebas de hipótesis constituyen la otra mitad. Tenga presente que, en el análisis de regresión, nuestro objetivo no sólo consiste en estimar la función de regresión muestral (FRM), sino también en utilizarla para obtener inferencias respecto de la función de regresión poblacional (FRP).

<br>

Por tanto, como $\hat{\beta_0}$, $\hat{\beta_1}$ y $\hat{\sigma^2}$ .orange[**son variables aleatorias,**] es necesario averiguar sus distribuciones de probabilidad, pues sin conocerlas no es posible relacionarlas con sus valores verdaderos.

]



---


# Distribución de probabilidad de las perturbaciones $u_i$

El modelo clásico de regresión lineal normal supone que cada $u_i$ está normalmente distribuida
con:

> * Media: $\quad \quad \quad \quad E(u_i) = 0$
> * Varianza: $\quad \quad \quad E(u_i -E(u_i))^2 = E(u_i^2) = \sigma^2$
> * Convarianza: $\quad  cov(u_i, u_j) = E[(u_i -E(u_i))(u_j -E(u_j))] = E(u_iu_j)=0 ; \quad i\neq j$

<br>

**De forma general,**

$$ u_i \sim N(0, \sigma^2)$$
----

ó, dónde $NID$ significa normal e independientemente distribuido.


$$ u_i \sim NID(0, \sigma^2)$$
----


---

# Propiedades de los estimadores de MCO bajo normalidad


> .font150[.orange[**1.**]] Son **insesgados**

> .font150[.orange[**2.**]] Tienen **varianza mínima**. En combinación con 1, esto significa que son estimadores insesgados con varianza mínima, o eficientes.

> .font150[.orange[**3.**]] Presentan **consistencia**; es decir, a medida que el tamaño de la muestra aumenta indefinidamente, los estimadores convergen hacia sus verdaderos valores poblacionales.

> .font150[.orange[**4.**]] $\hat{\beta_0}$ sigue una distribución normal, es decir:  $\hat{\beta_0} \sim N(\beta_0, \sigma^2_\hat{\beta_0})$.

> .font150[.orange[**5.**]] $\hat{\beta_1}$ sigue una distribución normal, es decir:  $\hat{\beta_1} \sim N(\beta_1, \sigma^2_\hat{\beta_1})$.



A propósito, observe que si supone que $u_i \sim N(0, \sigma^2)$, $Y_i$, al ser una función lineal de $u_i$, posee también una distribución normal con una media y una varianza dadas por:

$$Y_i \sim N(\beta_0 + \beta_2X_i, \sigma^2)$$

---



# Actividad en clase


.left-column[

Resulta que te han contratado para dirigir un proyecto en el área de Educación, con la finalidad de caracterizar parte de la población. 

<br>

A continuación hay una guía de como trabajar algunas variables en R de la base de datos de Educación de la Encuesta Nacional de Calidad de Vida.

]


.right-column[


.scroll-box-20[

``` r


library(haven)


Educ = read_dta("Educación.dta")

View(Educ)


hist(Educ$P6167)


library(dplyr)

# Realicemos conteos de la variable P4693:
# ¿Qué medio de transporte utiliza para ir a la institución donde estudia?

Educ %>% 
  count(P4693)


# Realicemos conteos de la variable P5673:
# ¿El establecimiento donde estudia es privado o público?


Educ %>% 
  count(P5673)


# Realicemos conteos conjuntos:

Educ %>% 
  count(P5673,P4693)


# Exploremos ahora las frecuencias relativas, tenemos que calcularlas
# Utilicemos la variable(P8586)
# ¿Usted actualmente estudia?

Educ %>% 
  count(P8586) %>% 
  mutate(Frecuencia = prop.table(n))


# Guardemos lo anterior en un objeto para poder graficarlo

A = Educ %>% 
    count(P8586) %>% 
    mutate(Frecuencia = prop.table(n))+
    mutate()



# Ahora me interes graficar el objeto "A" creado anteriormente


library(ggplot2)
library(labelled)


ggplot( data = A, aes(x= to_factor(P8586), y = Frecuencia)) +
  geom_bar(stat = "identity") +
  labs(x = "¿Usted actualmente estudia?" )
  

```
]

]



---


# Actividad en clase


.left-column[

![](https://media.giphy.com/media/1gdkWME574oWh9ESjc/giphy.gif)

]


.right-column[

Descargue la base de datos del siguiente link: [ECV 2018](http://microdatos.dane.gov.co/index.php/catalog/607/get_microdata)

<br>

* Sección de educación, o salud, o datos de la vivienda, o el módulo que usted quiera.
* Utilice la librería `haven` para cargar la base de datos (tal cual está el código guía)
* Identifique una relación teórica entre las dos variables y estime un modelo de regresión lineal simple.
* Asuma que la muestra que utilizó es la `población`, de tal manera, la estimación anterior es la FRP.
* Realice diferentes remuestreos del tamaño que usted considere y analice la distribución de los $\hat{\beta_i}$
* ¿Qué distribución siguen?


]






---

class: inverse, center, middle
background-color: #122140

.pull-left[

.center[
<br><br>

# Gracias!!!

<br>



### ¿Preguntas?

<br>


```{r qr, echo=FALSE, fig.align="center", out.width="49%"}
knitr::include_graphics("img/qr-code.png")
```


]


]


.pull-right[

<br> 
<br> 
<img style="border-radius: 50%;" src="img/avatar.png"
width="150px"
/>

### [www.joaquibarandica.com](https://www.joaquibarandica.com)

`r icon("envelope")` orlando.joaqui@javerianacali.edu.co

<img src="img/Logo.jpg" width="120%">

]


<br><br><br>









